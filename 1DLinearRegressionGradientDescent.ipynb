{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Linear Regression Using Gradient Descent\n",
    "\n",
    "Here we will introduce Multiple Linear Regression and compute the weights using Gradient Descent\n",
    "\n",
    "This notebook will give you an algorithm that will work for n number of input features but first we'll test it on 1D data\n",
    "\n",
    "Our process will be:\n",
    "\n",
    "* Loading the data\n",
    "* Exploring the data\n",
    "* Adding bias to our input \n",
    "* Solve for weights using Gradient Descent\n",
    "* Fit a line using weights\n",
    "* Make a Prediction\n",
    "\n",
    "We'll be using the following equations:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "the objective is to minimize the cost function $J(\\theta)$\n",
    "\n",
    "where $\\theta$ is computed using gradient descent\n",
    "\n",
    "and the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^T x = x_0\\theta_0 + \\theta_1 x_1$$\n",
    "\n",
    "the weights of the model are the $\\theta_{j}$ values and are adjusted to minimize $J(\\theta)$\n",
    "\n",
    "one way to do this is to use batch gradient descent where each iteration performs an update\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
